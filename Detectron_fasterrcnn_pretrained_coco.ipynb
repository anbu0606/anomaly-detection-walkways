{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Detectron2 is Facebooks new vision library \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.1 in c:\\users\\hp\\appdata\\roaming\\python\\python36\\site-packages (5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\HP\\anaconda3\\envs\\FaceRec\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: PyYAML\n",
      "Version: 5.1\n",
      "Summary: YAML parser and emitter for Python\n",
      "Home-page: https://github.com/yaml/pyyaml\n",
      "Author: Kirill Simonov\n",
      "Author-email: xi@resolvent.net\n",
      "License: MIT\n",
      "Location: c:\\users\\hp\\appdata\\roaming\\python\\python36\\site-packages\n",
      "Requires: \n",
      "Required-by: Keras, yacs, omegaconf, fvcore, dask, albumentations\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pyyaml==5.1\n",
    "!pip show pyyaml  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi\n",
    "!nvcc --version #10.1\n",
    "!python --version # 3.7.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Only\n",
    "#conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cpuonly -c pytorch\n",
    "\n",
    "# CUDA 10.1\n",
    "#!conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=10.1 -c pytorch\n",
    "\n",
    "#!pip3 install torch==1.8.0 torchvision==0.9.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n",
    "\n",
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0 False\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Requirements\n",
    "\n",
    "1. fvcore: pip install git+https://github.com/facebookresearch/fvcore\n",
    "\n",
    "2. pycocotools: pip install cython; pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/philferriere/cocoapi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: 'cocoapi/PythonAPI'\n",
      "C:\\Users\\HP\\Mini Project\\Detectron2-Tutorial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'setup.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%cd cocoapi/PythonAPI\n",
    "!python setup.py build_ext install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: '../../detectron2'\n",
      "C:\\Users\\HP\\Mini Project\\Detectron2-Tutorial\n"
     ]
    }
   ],
   "source": [
    "%cd ../../detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'setup.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python setup.py build develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference with pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pre-trained model is super easy in Detectron2. You only need to load in a config and some weights and then create a DefaultPredictor. After that you can simply make predictions and display them using Detectron's Visualizer utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some common detectron2 utilities\n",
    "from detectron2 import *\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "path=\"../dataset/Test004/\"\n",
    "im = cv2.imread(\"y.tif\")\n",
    "#cv2.imwrite(\"x.jpg\",img,[int(cv2.IMWRITE_JPEG_QUALITY), 200])\n",
    "#im = cv2.imread(path+\"x.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15bed750da0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the image with matplotlib\n",
    "#from IPython.display import Image\n",
    "#Image('../download.jpg')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,7.5))\n",
    "plt.imshow(im[0, ::-1]) #bgr to rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor\n",
    "\n",
    "# If getting AssertionError: Torch not compiled with CUDA enabled detectron2. Then do the below given line\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  0,  0,  0,  0,  0,  7,  0,  0,  0, 24,  0,  0, 24, 36])\n",
      "Boxes(tensor([[ 51.8325, 116.8473,  71.9855, 147.9277],\n",
      "        [ 29.1780, 119.0322,  41.0629, 150.8572],\n",
      "        [234.0138, 129.0095, 255.1849, 162.7054],\n",
      "        [148.2239, 114.6921, 166.0296, 149.9127],\n",
      "        [101.6209, 107.8644, 111.9267, 138.4512],\n",
      "        [339.2444,  95.0136, 351.4698, 123.8319],\n",
      "        [244.6751, 142.3254, 325.8720, 191.5563],\n",
      "        [301.5641, 100.5347, 312.3703, 129.8334],\n",
      "        [290.5341, 106.8128, 301.5305, 134.2334],\n",
      "        [194.1723, 212.8246, 210.5061, 240.0000],\n",
      "        [194.4763, 219.5479, 205.4425, 234.5256],\n",
      "        [150.5106, 104.5667, 164.1210, 135.2664],\n",
      "        [227.0921,  94.7503, 239.8811, 118.4814],\n",
      "        [246.9312, 136.5886, 253.4363, 146.9085],\n",
      "        [339.8276, 118.8889, 350.1174, 123.5877]]))\n"
     ]
    }
   ],
   "source": [
    "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
    "print(outputs[\"instances\"].pred_classes)\n",
    "print(outputs[\"instances\"].pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "\n",
    "def cv2_imshow(im):\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(25,7.5)), plt.imshow(im), plt.axis('off');\n",
    "plt.figure(figsize=(10,5))\n",
    "#print(out.get_image()[:, :, ::-1])\n",
    "cv2.imwrite(\"y1.jpg\",out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keypoint detection  : detecting people and localizing their keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from detectron2 import model_zoo\n",
    "\n",
    "# Inference with a keypoint detection model\n",
    "cfg = get_cfg()   # get a fresh new config\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "im = cv2.imread(\"y.tif\")\n",
    "\n",
    "def cv2_imshow(im):\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(25,7.5)), plt.imshow(im), plt.axis('off');\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)\n",
    "\n",
    "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "\n",
    "def cv2_imshow(im):\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(25,7.5)), plt.imshow(im), plt.axis('off');\n",
    "plt.figure(figsize=(10,5))\n",
    "#print(out.get_image()[:, :, ::-1])\n",
    "cv2.imwrite(\"y2.jpg\",out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panoptic segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n",
    "\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
    "cv2.imwrite(\"y3.jpg\",out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"y4.jpg\",out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
